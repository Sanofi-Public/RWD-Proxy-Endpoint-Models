{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-stage feature selection\n",
    "\n",
    "![End-to-end pipeline](../docs/overview_e2e.jpg)\n",
    "\n",
    "The multi-stage feature selection method integrates both data science and clinical knowledge to suggest feature candidates for downstream training of proxy models. It consists of multiple stages that are applied in sequential manner. Eventually, the selected features will contain all necessary properties (e.g. high fill rate, dependence with the target) which make them suitable for model training. The following selectors are included:\n",
    "\n",
    "- `MissingnessSelector` - selecting features based on **fill rate**\n",
    "- `DataTypeSelector` - selecting features based on **data type**\n",
    "- `VarianceThreshold` - selecting features based on **variance**\n",
    "- `RankingSelector` - selecting features based on **ranking method**\n",
    "    - `SelectFromRF` - selecting based on impurity obtained from shallow [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "    - `SelectFromXGB` - selecting based on impurity obtained from shallow [`XGBRegressor`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor)\n",
    "    - `SelectFromF_reg` - selecting based on [F-statistic](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "    - `SelectFromMutual` - selecting based on [mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.miual_info_regression.html)\n",
    "\n",
    "Each method from `RankingSelector` provides a separate list of feature candidates (`TOP_K` % features). We take the union of 4 sets to create the final feature shortlist. \n",
    "\n",
    "**Inputs**: \n",
    "\n",
    "Please specify `DATA_HOME` in the [\"Loading the data models\"](#Loading-the-data-models). Put input raw data `AD_final.csv`, `RA_final.csv` there in this folder.\n",
    "\n",
    "**Outputs**:\n",
    "\n",
    "- `AD_ppl_final.pkl` - processing pipeline for AD dataset, includes fitted methods for imputation, encoding, normalization\n",
    "- `RA_ppl_final.pkl` - processing pipeline for RA dataset, includes fitted methods for imputation, encoding, normalization\n",
    "- `AD_processed_final.csv` - processed AD dataset with features used for training\n",
    "- `RA_processed_final.csv` - processed RA dataset with features used for training\n",
    "- `ad_selected_features/ad_multi_stage_features_{TOP_K}.txt` - list of features selected for AD dataset\n",
    "- `ra_selected_features/ra_multi_stage_features_{TOP_K}.txt` - list of features selected for RA dataset\n",
    "\n",
    "## Table of contents\n",
    "    \n",
    " - [Multi-stage feature selection](#Multi-stage-feature-selection)<br>\n",
    "   - [Table of contents](#Table-of-contents)<br>\n",
    " - [Preparation](#Preparation)<br>\n",
    " - [Loading the data models](#Loading-the-data-models)<br>\n",
    " - [Feature dtype identification](#Feature-dtype-identification)<br>\n",
    " - [Data preprocessing](#Data-preprocessing)<br>\n",
    "   - [Preparation](#Preparation)<br>\n",
    "   - [Splitting data <a id='splitting-data'></a>](#Splitting-data-%3Ca-id%3D%27splitting-data%27%3E%3C/a%3E)<br>\n",
    "   - [Imputation](#Imputation)<br>\n",
    "     - [AD](#AD)<br>\n",
    "     - [RA](#RA)<br>\n",
    "   - [Category encoding](#Category-encoding)<br>\n",
    "   - [Normalization](#Normalization)<br>\n",
    "   - [Saving processed dataframe](#Saving-processed-dataframe)<br>\n",
    "   - [Getting `train` and `test` data sets as separate dataframes](#Getting-%60train%60-and-%60test%60-data-sets-as-separate-dataframes)<br>\n",
    " - [Multi-stage feature selection](#Multi-stage-feature-selection)<br>\n",
    "   - [`MissingnessSelector`](#%60MissingnessSelector%60)<br>\n",
    "   - [`DataTypeSelector`](#%60DataTypeSelector%60)<br>\n",
    "   - [`VarianceThreshold`](#%60VarianceThreshold%60)<br>\n",
    "   - [`RankingSelector`](#%60RankingSelector%60)<br>\n",
    "     - [`SelectFromRF`](#%60SelectFromRF%60)<br>\n",
    "     - [`SelectFromXGB`](#%60SelectFromXGB%60)<br>\n",
    "     - [`SelectFromF_reg`](#%60SelectFromF_reg%60)<br>\n",
    "     - [`SelectFromMutual`](#%60SelectFromMutual%60)<br>\n",
    " - [Taking the feature union between ranking method outputs](#Taking-the-feature-union-between-ranking-method-outputs)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "from functools import partial, update_wrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "\n",
    "# Adding the modules to the PYTHONPATH\n",
    "# add the path to your repository below\n",
    "REPO_PATH = \"\"\n",
    "sys.path.append(REPO_PATH)\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Top k % / 100 features to select according to each ranking method\n",
    "TOP_K = 0.05  # 0.05, 0.01, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to clinical trial datasets\n",
    "DATA_HOME = \"\"\n",
    "\n",
    "# Loading the data\n",
    "# AD_final.csv and RA_final.csv are studies processed according to data engineering methodology within publication\n",
    "ad_data = pd.read_csv(os.path.join(DATA_HOME, \"AD_final.csv\"))\n",
    "ra_data = pd.read_csv(os.path.join(DATA_HOME, \"RA_final.csv\"))\n",
    "\n",
    "# Setting up the target, patient ID column names\n",
    "ad_target_col, ad_patient_id = \"endpt_lb_easi1_total_score\", \"patient_id\"\n",
    "ra_target_col, ra_patient_id = \"endpt_lb_das28__crp_\", \"patient_id\"\n",
    "\n",
    "# Getting the feature sets for both use cases\n",
    "ad_data = ad_data.rename(columns={\"ft_sl_siteid\": \"site_id\"})\n",
    "ad_all_features = ad_data.filter(regex=\"^ft_\").columns.to_list()\n",
    "ra_all_features = ra_data.filter(regex=\"^ft_\").columns.to_list()\n",
    "\n",
    "# Drop records where target is NaN\n",
    "ad_data = ad_data.dropna(subset=[ad_target_col]).reset_index(drop=True)\n",
    "ra_data = ra_data.dropna(\n",
    "    subset=[ra_target_col, \"ft_lb_c_reactive_protein__mg_l_\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Creating block list of features for both datasets based on\n",
    "# (1) RWD availability; (2) discussion amongst clinical and technical teams\n",
    "ad_block_list = [\n",
    "    \"ft_sl_actarm\",\n",
    "    \"ft_sl_actarmcd\",\n",
    "    \"ft_sl_ageu\",\n",
    "    \"ft_sl_arm\",\n",
    "    \"ft_sl_armcd\",\n",
    "    \"ft_sl_domain\",\n",
    "    \"ft_sl_dthdtc\",\n",
    "    \"ft_sl_ethnic\",\n",
    "    \"ft_sl_invid\",\n",
    "    \"ft_sl_invnam\",\n",
    "    \"ft_sl_rfstdtc\",\n",
    "    \"ft_sl_rfendtc\",\n",
    "    \"ft_sl_rficdtc\",\n",
    "    \"ft_sl_rfpendtc\",\n",
    "    \"ft_sl_rfxendtc\",\n",
    "    \"ft_sl_rfxstdtc\",\n",
    "    \"ft_sl_rowid\",\n",
    "    \"ft_sl_dthfl\",\n",
    "    \"ft_sl_subjid\",\n",
    "    \"ft_sl_studyid\",\n",
    "] + ad_data.filter(regex=\"^ft_cc\").columns.to_list()\n",
    "ra_block_list = (\n",
    "    [\n",
    "        \"ft_sl_actarm\",\n",
    "        \"ft_sl_actarmcd\",\n",
    "        \"ft_sl_ageu\",\n",
    "        \"ft_sl_arm\",\n",
    "        \"ft_sl_armcd\",\n",
    "        \"ft_sl_domain\",\n",
    "        \"ft_sl_dthdtc\",\n",
    "        \"ft_sl_dthfl\",\n",
    "        \"ft_sl_ethnic\",\n",
    "        \"ft_sl_invid\",\n",
    "        \"ft_sl_invnam\",\n",
    "        \"ft_sl_rfendtc\",\n",
    "        \"ft_sl_rficdtc\",\n",
    "        \"ft_sl_rfpendtc\",\n",
    "        \"ft_sl_rfstdtc\",\n",
    "        \"ft_sl_rfxendtc\",\n",
    "        \"ft_sl_rfxstdtc\",\n",
    "        \"ft_sl_studyid\",\n",
    "        \"ft_sl_subjid\",\n",
    "        \"ft_sl_siteid\",\n",
    "    ]\n",
    "    + ra_data.filter(regex=\"ft_eff\").columns.to_list()\n",
    "    + [\"endpt_lb_das28__esr_\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD dataset contains {ad_data.shape[0]} samples and {ad_data.shape[1]} columns..\"\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    f\"RA dataset contains {ra_data.shape[0]} samples and {ra_data.shape[1]} columns..\"\n",
    ")\n",
    "print()\n",
    "print(\"AD dataset:\")\n",
    "display(ad_data.drop(columns=ad_block_list))\n",
    "print()\n",
    "print(\"RA dataset:\")\n",
    "display(ra_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature dtype identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No-feature columns\n",
    "ad_non_features = {ad_target_col, ad_patient_id, \"split\"}\n",
    "ra_non_features = {ra_target_col, ra_patient_id, \"split\"}\n",
    "\n",
    "# Creating inclusion lists\n",
    "ad_include_fs = [\n",
    "    f for f in ad_all_features if f not in ad_block_list and f not in ad_non_features\n",
    "]\n",
    "ra_include_fs = [\n",
    "    f for f in ra_all_features if f not in ra_block_list and f not in ra_non_features\n",
    "]\n",
    "\n",
    "# Specifying features based on data type\n",
    "ad_cat_features = [\n",
    "    f for f in ad_data.select_dtypes(object).columns.to_list() if f in ad_include_fs\n",
    "]\n",
    "ra_cat_features = [\n",
    "    f for f in ra_data.select_dtypes(object).columns.to_list() if f in ra_include_fs\n",
    "]\n",
    "\n",
    "ad_num_features = [f for f in ad_include_fs if f not in ad_cat_features]\n",
    "ra_num_features = [f for f in ra_include_fs if f not in ra_cat_features]\n",
    "\n",
    "# Creating final feature sets\n",
    "ad_features = set(ad_num_features) | set(\n",
    "    ad_cat_features\n",
    ")  # not including dt features now in `features`\n",
    "\n",
    "ra_features = set(ra_num_features) | set(\n",
    "    ra_cat_features\n",
    ")  # not including dt features now in `features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's write a method-agnostic function that will preprocess input data with `scikit-learn` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fit_transform(transformers, train_test_data, target_col):\n",
    "    \"\"\"Fit and transform data based on passed list of transformers.\"\"\"\n",
    "\n",
    "    processed_data = train_test_data.copy()\n",
    "\n",
    "    for tag, transformer, features in transformers:\n",
    "        transformer.fit(\n",
    "            processed_data.loc[processed_data[\"split\"] == \"TRAIN\", features],\n",
    "            processed_data.loc[processed_data[\"split\"] == \"TRAIN\", target_col],\n",
    "        )\n",
    "        processed_data[features] = transformer.transform(processed_data[features])\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "ad_ppl, ra_ppl = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data <a id='splitting-data'></a>\n",
    "\n",
    "Here we will split the original data into `train` and `test` sets by adding a column `\"split\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "\n",
    "def add_stratification_col(_data, _target):\n",
    "    \"\"\"Add a column that will stratify the target distribution by binning into 5 buckets.\"\"\"\n",
    "    bins = _data[_target].quantile([0, 0.25, 0.5, 0.75, 1]).to_numpy()\n",
    "    bins[-1] += 1\n",
    "    inds = np.digitize(_data[_target], bins)\n",
    "    _data[\"stratification_col\"] = [\n",
    "        f\"{bins[inds[i]-1]:.2f} <= {_target} < {bins[inds[i]]:.2f}\"\n",
    "        for i in range(_data.shape[0])\n",
    "    ]\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def make_splits(data, splitter_func, group_col_name, target_col_name):\n",
    "    \"\"\"Splits data into TRAIN and TEST with a passed splitter.\"\"\"\n",
    "\n",
    "    split_func_args = {\n",
    "        \"y\": data[target_col_name].copy(),\n",
    "        \"groups\": data[group_col_name].copy(),\n",
    "    }\n",
    "\n",
    "    indices = list(splitter_func.split(data, **split_func_args))\n",
    "    train_index, test_index = indices[0]\n",
    "    data.loc[train_index, \"split\"] = \"TRAIN\"\n",
    "    data.loc[test_index, \"split\"] = \"TEST\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "# AD\n",
    "# Separating data into train and test sets\n",
    "ad_data = add_stratification_col(ad_data, ad_target_col)\n",
    "ad_train_test_data = make_splits(\n",
    "    data=ad_data,\n",
    "    splitter_func=StratifiedGroupKFold(\n",
    "        n_splits=int(1 / test_size), random_state=SEED, shuffle=True\n",
    "    ),\n",
    "    target_col_name=\"stratification_col\",\n",
    "    group_col_name=ad_patient_id,\n",
    ")\n",
    "\n",
    "print(\"AD dataset:\")\n",
    "print(\n",
    "    f'   Training set: {ad_train_test_data.loc[lambda x: x[\"split\"] == \"TRAIN\"].shape[0]} samples'\n",
    ")\n",
    "print(\n",
    "    f'   Testing set: {ad_train_test_data.loc[lambda x: x[\"split\"] == \"TEST\"].shape[0]} samples'\n",
    ")\n",
    "print()\n",
    "\n",
    "# RA\n",
    "# Separating data into train and test sets\n",
    "ra_data = add_stratification_col(ra_data, ra_target_col)\n",
    "ra_train_test_data = make_splits(\n",
    "    data=ra_data,\n",
    "    splitter_func=StratifiedGroupKFold(\n",
    "        n_splits=int(1 / test_size), random_state=SEED, shuffle=True\n",
    "    ),\n",
    "    target_col_name=\"stratification_col\",\n",
    "    group_col_name=ra_patient_id,\n",
    ")\n",
    "\n",
    "print(\"RA dataset:\")\n",
    "print(\n",
    "    f'   Training set: {ra_train_test_data.loc[lambda x: x[\"split\"] == \"TRAIN\"].shape[0]} samples'\n",
    ")\n",
    "print(\n",
    "    f'   Testing set: {ra_train_test_data.loc[lambda x: x[\"split\"] == \"TEST\"].shape[0]} samples'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "We will start by removing features with extremely high missingness (>90 %)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# A percentage threshold to flag the features that will be dropped due to high level of missingness, must be a list (with at least 1 element)\n",
    "NA_THRESH = 0.9\n",
    "\n",
    "# Define feature set where imputation will be skipped according to `NA_THRESH`\n",
    "ad_na_skip_features = set(\n",
    "    (ad_train_test_data.isna().sum() / ad_train_test_data.shape[0])\n",
    "    .loc[lambda x: x > NA_THRESH]\n",
    "    .index\n",
    ")\n",
    "\n",
    "\n",
    "# Defining pipeline options\n",
    "ad_imputation_params = [\n",
    "    (\n",
    "        \"imp_Simple_cats\",\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        list(set(ad_cat_features) - set(ad_na_skip_features)),\n",
    "    ),\n",
    "    (\n",
    "        \"imp_Simple_nums\",\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        list(set(ad_num_features) - set(ad_na_skip_features)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"The following features will be skipped from the analysis due to missingness (if not saved by `free_pass_list`):\"\n",
    ")\n",
    "print(f\"   NA threshold = {NA_THRESH}: {ad_na_skip_features}\")\n",
    "print(ad_imputation_params)\n",
    "\n",
    "ad_imp_data = apply_fit_transform(\n",
    "    ad_imputation_params, ad_train_test_data, ad_target_col\n",
    ")\n",
    "\n",
    "ad_ppl[\"imp\"] = ad_imputation_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# A percentage threshold to flag the features that will be dropped due to high level of missingness, must be a list (with at least 1 element)\n",
    "NA_THRESH = 0.9\n",
    "\n",
    "# Define feature set where imputation will be skipped according to `NA_THRESH`\n",
    "ra_na_skip_features = set(\n",
    "    (ra_train_test_data.isna().sum() / ra_train_test_data.shape[0])\n",
    "    .loc[lambda x: x > NA_THRESH]\n",
    "    .index\n",
    ")\n",
    "\n",
    "\n",
    "# Defining pipeline options\n",
    "ra_imputation_params = [\n",
    "    (\n",
    "        \"imp_Simple_cats\",\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        list(set(ra_cat_features) - set(ra_na_skip_features)),\n",
    "    ),\n",
    "    (\n",
    "        \"imp_Simple_nums\",\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        list(set(ra_num_features) - set(ra_na_skip_features)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"The following features will be skipped from the analysis due to missingness (if not saved by `free_pass_list`):\"\n",
    ")\n",
    "print(f\"   NA threshold = {NA_THRESH}: {ra_na_skip_features}\")\n",
    "print(ra_imputation_params)\n",
    "\n",
    "ra_imp_data = apply_fit_transform(\n",
    "    ra_imputation_params, ra_train_test_data, ra_target_col\n",
    ")\n",
    "ra_ppl[\"imp\"] = ra_imputation_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Category encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "# AD\n",
    "print(\"AD dataset:\")\n",
    "ad_category_encoding_params = [\n",
    "    (\n",
    "        \"cat_enc_Target\",\n",
    "        TargetEncoder(),\n",
    "        list(set(ad_cat_features) - set(ad_na_skip_features)),\n",
    "    )\n",
    "]\n",
    "print(ad_category_encoding_params)\n",
    "ad_cat_enc_data = apply_fit_transform(\n",
    "    ad_category_encoding_params, ad_imp_data, ad_target_col\n",
    ")\n",
    "ad_ppl[\"cat_enc\"] = ad_category_encoding_params\n",
    "print()\n",
    "\n",
    "# RA\n",
    "print(\"RA dataset:\")\n",
    "ra_category_encoding_params = [\n",
    "    (\n",
    "        \"cat_enc_Target\",\n",
    "        TargetEncoder(),\n",
    "        list(set(ra_cat_features) - set(ra_na_skip_features)),\n",
    "    )\n",
    "]\n",
    "ra_cat_enc_data = apply_fit_transform(\n",
    "    ra_category_encoding_params, ra_imp_data, ra_target_col\n",
    ")\n",
    "ra_ppl[\"cat_enc\"] = ra_category_encoding_params\n",
    "print(ra_category_encoding_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# AD\n",
    "print(\"AD dataset:\")\n",
    "ad_normalization_params = [\n",
    "    (\n",
    "        \"norm_Robust\",\n",
    "        RobustScaler(),\n",
    "        list(set(ad_num_features) - set(ad_na_skip_features)),\n",
    "    )\n",
    "]\n",
    "print(ad_normalization_params)\n",
    "ad_norm_data = apply_fit_transform(\n",
    "    ad_normalization_params, ad_cat_enc_data, ad_target_col\n",
    ")\n",
    "ad_ppl[\"norm\"] = ad_normalization_params\n",
    "print()\n",
    "# RA\n",
    "print(\"RA dataset:\")\n",
    "ra_normalization_params = [\n",
    "    (\n",
    "        \"norm_Robust\",\n",
    "        RobustScaler(),\n",
    "        list(set(ra_num_features) - set(ra_na_skip_features)),\n",
    "    )\n",
    "]\n",
    "print(ra_normalization_params)\n",
    "ra_norm_data = apply_fit_transform(\n",
    "    ra_normalization_params, ra_cat_enc_data, ra_target_col\n",
    ")\n",
    "ra_ppl[\"norm\"] = ra_normalization_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(DATA_HOME, \"AD_processed_final.csv\")\n",
    "if os.path.exists(fn):\n",
    "    raise ValueError(f\"{fn} already exists in the data folder.\")\n",
    "else:\n",
    "    ad_norm_data.to_csv(fn, index=False)\n",
    "\n",
    "fn = os.path.join(DATA_HOME, \"RA_processed_final.csv\")\n",
    "if os.path.exists(fn):\n",
    "    raise ValueError(f\"{fn} already exists in the data folder.\")\n",
    "else:\n",
    "    ra_norm_data.to_csv(fn, index=False)\n",
    "\n",
    "write_pickle(ad_ppl, os.path.join(DATA_HOME, \"AD_ppl_final.pkl\"))\n",
    "write_pickle(ra_ppl, os.path.join(DATA_HOME, \"RA_ppl_final.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting `train` and `test` data sets as separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_features_to_use = [f for f in ad_features if f not in ad_na_skip_features]\n",
    "ra_features_to_use = [f for f in ra_features if f not in ra_na_skip_features]\n",
    "\n",
    "# Getting `X`s\n",
    "ad_X_train, ad_X_test = (\n",
    "    ad_norm_data.loc[ad_norm_data[\"split\"] == \"TRAIN\", ad_features_to_use],\n",
    "    ad_norm_data.loc[ad_norm_data[\"split\"] == \"TEST\", ad_features_to_use],\n",
    ")\n",
    "ra_X_train, ra_X_test = (\n",
    "    ra_norm_data.loc[ra_norm_data[\"split\"] == \"TRAIN\", ra_features_to_use],\n",
    "    ra_norm_data.loc[ra_norm_data[\"split\"] == \"TEST\", ra_features_to_use],\n",
    ")\n",
    "\n",
    "# Getting `groups`s\n",
    "ad_groups_train, ad_groups_test = (\n",
    "    ad_norm_data.loc[ad_norm_data[\"split\"] == \"TRAIN\", ad_patient_id],\n",
    "    ad_norm_data.loc[ad_norm_data[\"split\"] == \"TEST\", ad_patient_id],\n",
    ")\n",
    "ra_groups_train, ra_groups_test = (\n",
    "    ra_norm_data.loc[ra_norm_data[\"split\"] == \"TRAIN\", ra_patient_id],\n",
    "    ra_norm_data.loc[ra_norm_data[\"split\"] == \"TEST\", ra_patient_id],\n",
    ")\n",
    "\n",
    "# Creating `y`s\n",
    "ad_y_train, ad_y_test = (\n",
    "    ad_norm_data.loc[ad_norm_data[\"split\"] == \"TRAIN\", ad_target_col],\n",
    "    ad_norm_data.loc[ad_norm_data[\"split\"] == \"TEST\", ad_target_col],\n",
    ")\n",
    "ra_y_train, ra_y_test = (\n",
    "    ra_norm_data.loc[ra_norm_data[\"split\"] == \"TRAIN\", ra_target_col],\n",
    "    ra_norm_data.loc[ra_norm_data[\"split\"] == \"TEST\", ra_target_col],\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD dataset contains:\\n\\t\"\n",
    "    f\"TRAIN: {ad_X_train.shape[1]} features and {ad_X_train.shape[0]} samples!\\n\\t\"\n",
    "    f\"TEST: {ad_X_test.shape[1]} features and {ad_X_test.shape[0]} samples!\\n\\t\"\n",
    ")\n",
    "print(\n",
    "    f\"RA dataset contains:\\n\\t\"\n",
    "    f\"TRAIN: {ra_X_train.shape[1]} features and {ra_X_train.shape[0]} samples!\\n\\t\"\n",
    "    f\"TEST: {ra_X_test.shape[1]} features and {ra_X_test.shape[0]} samples!\\n\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-stage feature selection\n",
    "\n",
    "Let us implement each stage separately and then we will apply them sequentially and take the union at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MissingnessSelector`\n",
    "\n",
    "We will first drop all features that have fill rate less than 50 %. We need to calculate it on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A percentage threshold to flag the features that will be dropped due to high level of missingness, must be a list (with at least 1 element)\n",
    "MISSING_THRESH = 0.5\n",
    "\n",
    "# Define feature set where imputation will be skipped according to `MISSING_THRESH`\n",
    "ad_features_to_drop = set(\n",
    "    (ad_train_test_data.isna().sum() / ad_train_test_data.shape[0])\n",
    "    .loc[lambda x: x > MISSING_THRESH]\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Define feature set where imputation will be skipped according to `MISSING_THRESH`\n",
    "ra_features_to_drop = set(\n",
    "    (ra_train_test_data.isna().sum() / ra_train_test_data.shape[0])\n",
    "    .loc[lambda x: x > MISSING_THRESH]\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Dropping features\n",
    "ad_X_train_present, ad_X_test_present = (\n",
    "    ad_X_train.drop(columns=ad_features_to_drop, errors=\"ignore\"),\n",
    "    ad_X_test.drop(columns=ad_features_to_drop, errors=\"ignore\"),\n",
    ")\n",
    "ra_X_train_present, ra_X_test_present = (\n",
    "    ra_X_train.drop(columns=ra_features_to_drop, errors=\"ignore\"),\n",
    "    ra_X_test.drop(columns=ra_features_to_drop, errors=\"ignore\"),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train.shape[1]} to {ad_X_train_present.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train.shape[1]} to {ra_X_train_present.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataTypeSelector`\n",
    "\n",
    "Let us select only numerical features as selected ML algorithms only handle numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only float numbers\n",
    "ad_X_train_num, ad_X_test_num = (\n",
    "    ad_X_train_present.select_dtypes(float),\n",
    "    ad_X_test_present.select_dtypes(float),\n",
    ")\n",
    "ra_X_train_num, ra_X_test_num = (\n",
    "    ra_X_train_present.select_dtypes(float),\n",
    "    ra_X_test_present.select_dtypes(float),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train_present.shape[1]} to {ad_X_train_num.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train_present.shape[1]} to {ra_X_train_num.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VarianceThreshold`\n",
    "\n",
    "Let us select only features with high-enough variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "VAR_THRESH = 0.1\n",
    "\n",
    "# AD\n",
    "var_selector = VarianceThreshold()\n",
    "var_selector.fit(ad_X_train_num)\n",
    "ad_X_train_var, ad_X_test_var = (\n",
    "    pd.DataFrame(\n",
    "        var_selector.transform(ad_X_train_num),\n",
    "        columns=var_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        var_selector.transform(ad_X_test_num),\n",
    "        columns=var_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # RA\n",
    "var_selector = VarianceThreshold()\n",
    "var_selector.fit(ra_X_train_num)\n",
    "ra_X_train_var, ra_X_test_var = (\n",
    "    pd.DataFrame(\n",
    "        var_selector.transform(ra_X_train_num),\n",
    "        columns=var_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        var_selector.transform(ra_X_test_num),\n",
    "        columns=var_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train_num.shape[1]} to {ad_X_train_var.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train_num.shape[1]} to {ra_X_train_var.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RankingSelector`\n",
    "\n",
    "Now we are ready to apply ranking selectors and take the feature union based on 4 outputs. Each method will select the top 5% of features based on its ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SelectFromRF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# AD\n",
    "ad_top_k = int(np.ceil(TOP_K * ad_X_train_var.shape[1]))\n",
    "rf_selector = SelectFromModel(\n",
    "    RandomForestRegressor(n_estimators=10, random_state=SEED),\n",
    "    threshold=1e-5,\n",
    "    importance_getter=partial(importance_getter_k, k=ad_top_k),\n",
    ")\n",
    "rf_selector.fit(ad_X_train_var, ad_y_train)\n",
    "ad_X_train_rf, ad_X_test_rf = (\n",
    "    pd.DataFrame(\n",
    "        rf_selector.transform(ad_X_train_var),\n",
    "        columns=rf_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        rf_selector.transform(ad_X_test_var),\n",
    "        columns=rf_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # RA\n",
    "ra_top_k = int(np.ceil(TOP_K * ra_X_train_var.shape[1]))\n",
    "rf_selector = SelectFromModel(\n",
    "    RandomForestRegressor(n_estimators=10, random_state=SEED),\n",
    "    threshold=1e-5,\n",
    "    importance_getter=partial(importance_getter_k, k=ra_top_k),\n",
    ")\n",
    "rf_selector.fit(ra_X_train_var, ra_y_train)\n",
    "ra_X_train_rf, ra_X_test_rf = (\n",
    "    pd.DataFrame(\n",
    "        rf_selector.transform(ra_X_train_var),\n",
    "        columns=rf_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        rf_selector.transform(ra_X_test_var),\n",
    "        columns=rf_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train_var.shape[1]} to {ad_X_train_rf.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train_var.shape[1]} to {ra_X_train_rf.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SelectFromXGB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# AD\n",
    "ad_top_k = int(np.ceil(TOP_K * ad_X_train_var.shape[1]))\n",
    "xgb_selector = SelectFromModel(\n",
    "    XGBRegressor(n_estimators=10, random_state=SEED),\n",
    "    threshold=1e-5,\n",
    "    importance_getter=partial(importance_getter_k, k=ad_top_k),\n",
    ")\n",
    "xgb_selector.fit(ad_X_train_var, ad_y_train)\n",
    "ad_X_train_xgb, ad_X_test_xgb = (\n",
    "    pd.DataFrame(\n",
    "        xgb_selector.transform(ad_X_train_var),\n",
    "        columns=xgb_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        xgb_selector.transform(ad_X_test_var),\n",
    "        columns=xgb_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# RA\n",
    "ra_top_k = int(np.ceil(TOP_K * ra_X_train_var.shape[1]))\n",
    "xgb_selector = SelectFromModel(\n",
    "    XGBRegressor(n_estimators=10, random_state=SEED),\n",
    "    threshold=1e-5,\n",
    "    importance_getter=partial(importance_getter_k, k=ra_top_k),\n",
    ")\n",
    "xgb_selector.fit(ra_X_train_var, ra_y_train)\n",
    "ra_X_train_xgb, ra_X_test_xgb = (\n",
    "    pd.DataFrame(\n",
    "        xgb_selector.transform(ra_X_train_var),\n",
    "        columns=xgb_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        xgb_selector.transform(ra_X_test_var),\n",
    "        columns=xgb_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train_var.shape[1]} to {ad_X_train_xgb.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train_var.shape[1]} to {ra_X_train_xgb.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SelectFromF_reg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# AD\n",
    "ad_top_k = int(np.ceil(TOP_K * ad_X_train_var.shape[1]))\n",
    "f_selector = SelectKBest(f_regression, k=ad_top_k)\n",
    "f_selector.fit(ad_X_train_var, ad_y_train)\n",
    "ad_X_train_f, ad_X_test_f = (\n",
    "    pd.DataFrame(\n",
    "        f_selector.transform(ad_X_train_var), columns=f_selector.get_feature_names_out()\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        f_selector.transform(ad_X_test_var), columns=f_selector.get_feature_names_out()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# RA\n",
    "ra_top_k = int(np.ceil(TOP_K * ra_X_train_var.shape[1]))\n",
    "f_selector = SelectKBest(f_regression, k=ra_top_k)\n",
    "f_selector.fit(ra_X_train_var, ra_y_train)\n",
    "ra_X_train_f, ra_X_test_f = (\n",
    "    pd.DataFrame(\n",
    "        f_selector.transform(ra_X_train_var), columns=f_selector.get_feature_names_out()\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        f_selector.transform(ra_X_test_var), columns=f_selector.get_feature_names_out()\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train_var.shape[1]} to {ad_X_train_f.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train_var.shape[1]} to {ra_X_train_f.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SelectFromMutual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "# AD\n",
    "ad_top_k = int(np.ceil(TOP_K * ad_X_train_var.shape[1]))\n",
    "fixed_seed_mi = update_wrapper(\n",
    "    partial(mutual_info_regression, random_state=SEED), mutual_info_regression\n",
    ")\n",
    "mi_selector = SelectKBest(fixed_seed_mi, k=ad_top_k)\n",
    "mi_selector.fit(ad_X_train_var, ad_y_train)\n",
    "ad_X_train_mi, ad_X_test_mi = (\n",
    "    pd.DataFrame(\n",
    "        mi_selector.transform(ad_X_train_var),\n",
    "        columns=mi_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        mi_selector.transform(ad_X_test_var),\n",
    "        columns=mi_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# RA\n",
    "ra_top_k = int(np.ceil(TOP_K * ra_X_train_var.shape[1]))\n",
    "fixed_seed_mi = update_wrapper(\n",
    "    partial(mutual_info_regression, random_state=SEED), mutual_info_regression\n",
    ")\n",
    "mi_selector = SelectKBest(fixed_seed_mi, k=ra_top_k)\n",
    "mi_selector.fit(ra_X_train_var, ra_y_train)\n",
    "ra_X_train_mi, ra_X_test_mi = (\n",
    "    pd.DataFrame(\n",
    "        mi_selector.transform(ra_X_train_var),\n",
    "        columns=mi_selector.get_feature_names_out(),\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        mi_selector.transform(ra_X_test_var),\n",
    "        columns=mi_selector.get_feature_names_out(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"AD: the number of features went from {ad_X_train_var.shape[1]} to {ad_X_train_mi.shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"RA: the number of features went from {ra_X_train_var.shape[1]} to {ra_X_train_mi.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking the feature union between ranking method outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AD\n",
    "ad_multi_stage_features = list(\n",
    "    set().union(\n",
    "        *[\n",
    "            ad_X_train_rf.columns.to_list(),\n",
    "            ad_X_train_xgb.columns.to_list(),\n",
    "            ad_X_train_f.columns.to_list(),\n",
    "            ad_X_train_mi.columns.to_list(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(\"AD features:\")\n",
    "for pref in [\"ft_sl\", \"ft_lb\", \"ft_mh\", \"ft_cm\"]:\n",
    "    curr_fts = [\n",
    "        t.replace(f\"{pref}_\", \"\") for t in ad_multi_stage_features if t.startswith(pref)\n",
    "    ]\n",
    "    print(f\"{pref} ({len(curr_fts)}): {', '.join(curr_fts)}\")\n",
    "    print()\n",
    "\n",
    "# Save the features to a file in DATA_HOME/ad_selected_features\n",
    "fn = os.path.join(\n",
    "    DATA_HOME, \"ad_selected_features\", f\"ad_multi_stage_features_{TOP_K}.txt\"\n",
    ")\n",
    "write_list(ad_multi_stage_features, fn)\n",
    "\n",
    "# RA\n",
    "ra_multi_stage_features = list(\n",
    "    set().union(\n",
    "        *[\n",
    "            ra_X_train_rf.columns.to_list(),\n",
    "            ra_X_train_xgb.columns.to_list(),\n",
    "            ra_X_train_f.columns.to_list(),\n",
    "            ra_X_train_mi.columns.to_list(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print()\n",
    "print()\n",
    "print(\"RA features:\")\n",
    "for pref in [\"ft_sl\", \"ft_lb\", \"ft_mh\", \"ft_cm\"]:\n",
    "    curr_fts = [\n",
    "        t.replace(f\"{pref}_\", \"\") for t in ra_multi_stage_features if t.startswith(pref)\n",
    "    ]\n",
    "    print(f\"{pref} ({len(curr_fts)}): {', '.join(curr_fts)}\")\n",
    "    print()\n",
    "# Save the features to a file in DATA_HOME/ra_selected_features\n",
    "fn = os.path.join(\n",
    "    DATA_HOME, \"ra_selected_features\", f\"ra_multi_stage_features_{TOP_K}.txt\"\n",
    ")\n",
    "write_list(ra_multi_stage_features, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_in_each_group(features, name):\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"sl\": len([f for f in features if f.startswith(\"ft_sl\")]),\n",
    "            \"lb\": len([f for f in features if f.startswith(\"ft_lb\")]),\n",
    "            \"mh\": len([f for f in features if f.startswith(\"ft_mh\")]),\n",
    "            \"cm\": len([f for f in features if f.startswith(\"ft_cm\")]),\n",
    "        },\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "out = (\n",
    "    pd.DataFrame(columns=[\"sl\", \"lb\", \"mh\", \"cm\"])\n",
    "    .append(calculate_number_in_each_group(ad_X_train.columns, \"Initial Set\"))\n",
    "    .append(\n",
    "        calculate_number_in_each_group(\n",
    "            ad_X_train_present.columns, \"Availability Threshold\"\n",
    "        )\n",
    "    )\n",
    "    .append(calculate_number_in_each_group(ad_X_train_num.columns, \"Data Type Filter\"))\n",
    "    .append(\n",
    "        calculate_number_in_each_group(ad_X_train_var.columns, \"Variance Threshold\")\n",
    "    )\n",
    "    .append(calculate_number_in_each_group(ad_X_train_mi.columns, \"Mutual Information\"))\n",
    "    .append(calculate_number_in_each_group(ad_X_train_f.columns, \"F-Test\"))\n",
    "    .append(calculate_number_in_each_group(ad_X_train_rf.columns, \"Random Forest\"))\n",
    "    .append(calculate_number_in_each_group(ad_X_train_xgb.columns, \"XGBoost\"))\n",
    "    .append(calculate_number_in_each_group(ad_multi_stage_features, \"Multi-Stage\"))\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_in_each_group(features, name):\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"sl\": len([f for f in features if f.startswith(\"ft_sl\")]),\n",
    "            \"lb\": len([f for f in features if f.startswith(\"ft_lb\")]),\n",
    "            \"mh\": len([f for f in features if f.startswith(\"ft_mh\")]),\n",
    "            \"cm\": len([f for f in features if f.startswith(\"ft_cm\")]),\n",
    "        },\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "out = (\n",
    "    pd.DataFrame(columns=[\"sl\", \"lb\", \"mh\", \"cm\"])\n",
    "    .append(calculate_number_in_each_group(ra_X_train.columns, \"Initial Set\"))\n",
    "    .append(\n",
    "        calculate_number_in_each_group(\n",
    "            ra_X_train_present.columns, \"Availability Threshold\"\n",
    "        )\n",
    "    )\n",
    "    .append(calculate_number_in_each_group(ra_X_train_num.columns, \"Data Type Filter\"))\n",
    "    .append(\n",
    "        calculate_number_in_each_group(ra_X_train_var.columns, \"Variance Threshold\")\n",
    "    )\n",
    "    .append(calculate_number_in_each_group(ra_X_train_mi.columns, \"Mutual Information\"))\n",
    "    .append(calculate_number_in_each_group(ra_X_train_f.columns, \"F-Test\"))\n",
    "    .append(calculate_number_in_each_group(ra_X_train_rf.columns, \"Random Forest\"))\n",
    "    .append(calculate_number_in_each_group(ra_X_train_xgb.columns, \"XGBoost\"))\n",
    "    .append(calculate_number_in_each_group(ra_multi_stage_features, \"Multi-Stage\"))\n",
    ")\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('proxy_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5546cda908818f46075379ccf437c01b71ba37d3306bb47673a7b21af4c78453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
